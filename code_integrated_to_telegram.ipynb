{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pybot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import tensorflow.keras.preprocessing.text as tftext\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bot_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {\n",
    "    'DATA_BATCH_SIZE' : 128,\n",
    "    'DATA_BUFFER_SIZE' : 10000, \n",
    "    'EMBEDDING_DIMENSION' : 100,  # Dimension of the GloVe embedding vector\n",
    "    'MAX_SENT_LENGTH' : 11,      # Maximum length of sentences\n",
    "    'MAX_SAMPLES' : 2000000,        # Maximum samples to consider (useful for laptop memory)  200000\n",
    "    'MIN_WORD_OCCURENCE' : 30,    # Minimum word count. If condition not met word replaced with <UNK>\n",
    "    'MODEL_LAYER_DIMS' : 500,\n",
    "    'MODEL_LEARN_RATE' : 1e-3,\n",
    "    'MODEL_LEARN_EPOCHS' : 10,\n",
    "    'MODEL_TRAINING' : False # When True, the model will retrain. False: The weights are loaded from file\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the data set\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "movie_lines:\n",
    "- lineID\n",
    "- characterID (who uttered this phrase)\n",
    "- movieID\n",
    "- character name\n",
    "- text of the utterance\n",
    "\n",
    "movie_conversations:\n",
    "- characterID of the first character involved in the conversation\n",
    "- characterID of the second character involved in the conversation\n",
    "- movieID of the movie in which the conversation occurred\n",
    "- list of the utterances that make the conversation, in chronological \n",
    "order: ['lineID1','lineID2',Ã‰,'lineIDN']\n",
    "has to be matched with movie_lines.txt to reconstruct the actual content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing the data\n",
    "- Perform cleaning\n",
    "- Extract Q&As\n",
    "- Tokenize\n",
    "- Add padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_to_lower(text):\n",
    "    \"\"\"Performs cleaning and lowers text\"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Expand contractions to words, and remove characters\n",
    "    for word in text.split():\n",
    "        if word in bot_utils.CONTRACTIONS:\n",
    "            text = text.replace(word, bot_utils.CONTRACTIONS[word]) \n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "    \n",
    "def get_questions_answers(lines, conversations):\n",
    "    \"\"\"Builds question and answers from data sets\"\"\"\n",
    "    # Map id to text of the utterance\n",
    "    id_to_line = {}\n",
    "    for line in lines:\n",
    "        line_ = line.split(' +++$+++ ')\n",
    "        if len(line_) == 5:\n",
    "            id_to_line[line_[0]] = line_[4]\n",
    "    \n",
    "    questions, answers = [], []\n",
    "    \n",
    "    for line in conversations:\n",
    "        # Extract conversation turns\n",
    "        conversation = line.split(' +++$+++ ')\n",
    "        if len(conversation) == 4:\n",
    "            conversation_turns = conversation[3][1:-1].split(', ')\n",
    "            turns_list = [turn[1:-1] for turn in conversation_turns]   \n",
    "            # Split to Q & A\n",
    "            for i_turn in range(len(conversation_turns) - 1):  \n",
    "                questions.append(clean_text_to_lower(id_to_line[conversation_turns[i_turn][1:-1]]))\n",
    "                answers.append(clean_text_to_lower(id_to_line[conversation_turns[i_turn + 1][1:-1]]))\n",
    "                if len(questions) >= HYPER_PARAMS['MAX_SAMPLES']:\n",
    "                    return questions, answers\n",
    "    return questions, answers\n",
    "\n",
    "def tokenize(lines, conversations):\n",
    "    \"\"\"Tokenizes sets to sequences of integers, and adds special tokens.\n",
    "    Also reduces the vocabulary by replacing low frequency words with an unknown token.\n",
    "    \n",
    "    Returns:\n",
    "    tokenizer: tokenizer,  which might be used later to reverse integers to words,\n",
    "    tokenized_questions: list, questions as sequence of integers\n",
    "    tokenized_answers: list, answer as sequence of integers\n",
    "    size_vocab: int, unique number of words in vocabulary.\n",
    "    special_tokens: dict, mappings for special tokens\"\"\"\n",
    "    \n",
    "    questions, answers = get_questions_answers(lines, conversations)\n",
    "    tokenizer = tftext.Tokenizer(oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(questions + answers)\n",
    "    \n",
    "    # Make UNK tokens, reindex tokenizer dicts\n",
    "    sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    tokenizer.word_index = {}\n",
    "    tokenizer.index_word = {}\n",
    "    index = 1\n",
    "    for word, count in sorted_by_word_count:\n",
    "        if count >= HYPER_PARAMS['MIN_WORD_OCCURENCE']:\n",
    "            tokenizer.word_index[word] = index\n",
    "            tokenizer.index_word[index] = word\n",
    "            index += 1\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = {}\n",
    "    special_tokens['<PAD>'] = 0 \n",
    "    special_tokens['<UNK>'] = len(tokenizer.word_index)\n",
    "    special_tokens['<SOS>'] = special_tokens['<UNK>'] + 1\n",
    "    special_tokens['<EOS>'] = special_tokens['<SOS>'] + 1\n",
    "    \n",
    "    for special_token, index_value in special_tokens.items():\n",
    "        tokenizer.word_index[special_token] = index_value\n",
    "        tokenizer.index_word[index_value] = special_token\n",
    "    \n",
    "    # Tokenize to integer sequences\n",
    "    tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "    tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "    \n",
    "    # Size is equal to the last token's index\n",
    "    size_vocab = special_tokens['<EOS>'] + 1\n",
    "\n",
    "    # Add sentence position tokens\n",
    "    tokenized_questions = [[special_tokens['<SOS>']] + tokenized_question + [special_tokens['<EOS>']]\n",
    "                           for tokenized_question in tokenized_questions]\n",
    "    \n",
    "    tokenized_answers   = [[special_tokens['<SOS>']] + tokenized_answer + [special_tokens['<EOS>']]\n",
    "                           for tokenized_answer in tokenized_answers]\n",
    "    \n",
    "    # Add padding at end so we can use a static input size to the model\n",
    "    tokenized_questions = pad_sequences(tokenized_questions, \n",
    "                                        maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                        padding='post')\n",
    "    tokenized_answers   = pad_sequences(tokenized_answers, \n",
    "                                        maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                        padding='post')\n",
    "    \n",
    "    \n",
    "    return tokenizer, tokenized_questions, tokenized_answers, size_vocab, special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, tokenized_questions, tokenized_answers,\\\n",
    "size_vocab, special_tokens = tokenize(lines, conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6492"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221616"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tf.data.Dataset** <br>\n",
    "Allows caching and prefetching to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'encoder_inputs': tokenized_questions[:, 1:], # Skip <SOS> token\n",
    "        'decoder_inputs': tokenized_answers[:, :-1] # Skip <EOS> token\n",
    "    },\n",
    "    {\n",
    "        'outputs': tokenized_answers[:, 1:]  # Skip <SOS> token\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(HYPER_PARAMS['DATA_BUFFER_SIZE'])\n",
    "dataset = dataset.batch(HYPER_PARAMS['DATA_BATCH_SIZE'])\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create contextual embedding from GloVe\n",
    "Create embedding layer with out context, from a pre-trained Word2Vec Model from Glove:<br>\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_weights(dimension_embedding):\n",
    "    \"\"\" Load GloVe pre-trained model\"\"\"\n",
    "    path='glove.6B.' + str(dimension_embedding) + 'd.txt'\n",
    "    word_to_vec = {}\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word_to_vec[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "        file.close()\n",
    "    return word_to_vec\n",
    "\n",
    "def load_embedding_weights(word_dictionary, size_vocab, dimension_embedding):\n",
    "    \"\"\"Loads embedding weights from GloVe based on our context\"\"\"\n",
    "    word_to_vec = load_glove_weights(dimension_embedding)\n",
    "    embedding_matrix = np.zeros((size_vocab,\n",
    "                                HYPER_PARAMS['EMBEDDING_DIMENSION']))\n",
    "    for word, index in word_dictionary.items():\n",
    "        embedding_vector = word_to_vec.get(word)\n",
    "        # Word is within GloVe dictionary\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix, word_to_vec\n",
    "\n",
    "def build_embedding_layer(word_dictionary, size_vocab, dimension_embedding, length_input): \n",
    "    \"\"\"Builds a non-trainable embedding layer from GloVe \n",
    "    pre-trained model, based on our context\"\"\"\n",
    "    embedding_matrix, word_to_vec  = load_embedding_weights(word_dictionary, size_vocab, dimension_embedding)\n",
    "    size_vocab = size_vocab\n",
    "    embedding_layer = Embedding(size_vocab,\n",
    "                                dimension_embedding,\n",
    "                                input_length=length_input,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "    return embedding_matrix, word_to_vec, embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, word_to_vec, embedding_layer = build_embedding_layer(tokenizer.word_index, \n",
    "                                                                       size_vocab,\n",
    "                                                                       HYPER_PARAMS['EMBEDDING_DIMENSION'],\n",
    "                                                                       HYPER_PARAMS['MAX_SENT_LENGTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(embedding_layer, length_input, size_vocab, layer_dims):\n",
    "    \"\"\"LSTM Seq2Seq model with attention\"\"\"\n",
    "    length_input = length_input - 1\n",
    "    # \n",
    "    # Encoder,\n",
    "    # Biderectional (RNNSearch) as explained in https://arxiv.org/pdf/1409.0473.pdf \n",
    "    #\n",
    "    encoder_inputs = Input(shape=(length_input, ), name='encoder_inputs')\n",
    "    encoder_embedding = embedding_layer(encoder_inputs)\n",
    "    ecoder_lstm = Bidirectional(LSTM(layer_dims,\n",
    "                                     return_state=True,\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=0.05,\n",
    "                                     recurrent_initializer='glorot_uniform',\n",
    "                                     name='encoder_lstm'),\n",
    "                                name='encoder_bidirectional')\n",
    "    \n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = ecoder_lstm(encoder_embedding)\n",
    "    # For annotating sequences, we concatenate forward hidden state with backward one as explained in top paper\n",
    "    state_h = Concatenate(name='encoder_hidden_state')([forward_h, backward_h])\n",
    "    state_c = Concatenate(name='encoder_cell_state')([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # \n",
    "    # Decoder,\n",
    "    # Unidirectional\n",
    "    #\n",
    "    decoder_inputs = Input(shape=(length_input, ), name='decoder_inputs')\n",
    "    decoder_embedding = embedding_layer(decoder_inputs)\n",
    "    decoder_lstm = LSTM(layer_dims * 2, # to match bidirectional size\n",
    "                        return_state=True,\n",
    "                        return_sequences=True,\n",
    "                        dropout=0.05,\n",
    "                        recurrent_initializer='glorot_uniform',\n",
    "                        name='decoder_lstm')\n",
    "    # Set encoder to use the encoder state as initial states\n",
    "    decoder_output,_ , _, = decoder_lstm(decoder_embedding,\n",
    "                                         initial_state=encoder_states)\n",
    "    \n",
    "    # Attention\n",
    "    attention_layer = AttentionLayer(name='attention_layer')\n",
    "    attention_output, attention_state = attention_layer([encoder_outputs, decoder_output])\n",
    "    decoder_concat = Concatenate(axis=-1)([decoder_output, attention_output])\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(size_vocab, name='outputs', activation='softmax')(decoder_concat)\n",
    "    \n",
    "    return Model([encoder_inputs, decoder_inputs], outputs), decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masked loss** <br>\n",
    "Since we are dealing with batches of padded sequences, we cannot simply consider all elements of the tensor when calculating loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, decoder_embedding = seq2seq(embedding_layer, \n",
    "                                   HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                   size_vocab,\n",
    "                                   HYPER_PARAMS['MODEL_LAYER_DIMS'])\n",
    "\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS['MODEL_LEARN_RATE'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5cb4b470ac3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n\u001b[0m\u001b[0;32m      2\u001b[0m                  rankdir='TB', dpi=65).create(prog='dot', format='svg'))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "# SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "#                  rankdir='TB', dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Optionally load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 really\n",
    "if HYPER_PARAMS['MODEL_TRAINING']:\n",
    "    model.fit(dataset, epochs=HYPER_PARAMS['MODEL_LEARN_EPOCHS'], batch_size=HYPER_PARAMS['DATA_BATCH_SIZE'])\n",
    "    model.save('backup_{0}.h5'.format(HYPER_PARAMS['MODEL_LEARN_EPOCHS']))\n",
    "else:\n",
    "    model.load_weights('backup_{0}.h5'.format(HYPER_PARAMS['MODEL_LEARN_EPOCHS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predictions processing\n",
    "- Follow the same tokeninzing approach as before\n",
    "- For words that are not in our original context, we use the GloVe embedding to find the most similar word within our vocabulary. If it is still a weird word, it will be replaced by an unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known_words(sentence_words, glove_word_dictionary, local_word_dictionary, topn=20):\n",
    "    \"\"\"Pre-process the input text, corrects spelling, \n",
    "    get similar words from gensim if input word is out of context\n",
    "    or replace with <UNK> for bad words.\"\"\"\n",
    "    result = []\n",
    "    for word in sentence_words:\n",
    "        \n",
    "        # Correct any spelling mistakes\n",
    "        spell = SpellChecker()\n",
    "        word = spell.correction(word)\n",
    "        \n",
    "        if word in local_word_dictionary:\n",
    "            result.append(word) \n",
    "        else:\n",
    "            # Determine if top match is within our local context\n",
    "            found_sim_word = False\n",
    "            if word in glove_word_dictionary:\n",
    "                similar_words = glove_vectors.similar_by_word(word, topn=topn)\n",
    "                for (sim_word, measure) in similar_words:\n",
    "                    if sim_word in local_word_dictionary.keys():\n",
    "                        found_sim_word = True\n",
    "                        result.append(sim_word)\n",
    "                        break\n",
    "            # Funny word\n",
    "            if not found_sim_word:\n",
    "                result.append('<UNK>')\n",
    "    return result\n",
    "            \n",
    "def pre_process_new_questions(text, tokenizer, glove_vectors, special_tokens):\n",
    "    \"\"\" Process text to words within our context,\n",
    "    and tokenize.\"\"\"\n",
    "    # Pre-process\n",
    "    text_ = clean_text_to_lower(text)\n",
    "    split_text = text_.split(' ')\n",
    "    split_text = get_known_words(split_text, glove_vectors, tokenizer.word_index)\n",
    "    processed_question = \" \".join(split_text)\n",
    "    # Tokenize to sequence of ints\n",
    "    # using original tokenizer\n",
    "    tokenized_question = tokenizer.texts_to_sequences([processed_question])\n",
    "    tokenized_question = [[special_tokens['<SOS>']] + \n",
    "                          tokenized_question[0] + \n",
    "                          [special_tokens['<EOS>']]]\n",
    "\n",
    "    # Padding                                                                     \n",
    "    tokenized_question = pad_sequences(tokenized_question, \n",
    "                                       maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                       padding='post')                                                                       \n",
    "    return processed_question, tokenized_question[0]\n",
    "\n",
    "def post_process_new_answers(text_sequence, tokenizer, glove_vectors):\n",
    "    answer = []\n",
    "    # Build string from text sequence\n",
    "    for text_id in text_sequence:\n",
    "        if text_id != tokenizer.word_index['<EOS>'] and\\\n",
    "           text_id != tokenizer.word_index['<PAD>']:\n",
    "            answer.append(tokenizer.index_word[text_id])\n",
    "    answer = \" \".join(answer)\n",
    "    \n",
    "    # TODO : ADD CBOW/SKIPGRAMS TO FIX <UNK> predictions  (find best word for surrounding words)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of correction** <br>\n",
    "Vanish is not within our model's context, but dissapear is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to disappear'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_question, _ = pre_process_new_questions('i wouzld like tso vansish', tokenizer, glove_vectors, special_tokens) \n",
    "processed_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_decoder(model):\n",
    "    \"\"\" Get the encoder and decoder models,\n",
    "        used to do inference of text sequences\"\"\"\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder_model = Model(model.get_layer('encoder_inputs').output,\n",
    "                          [model.get_layer('encoder_bidirectional').output[0],\n",
    "                          [model.get_layer('encoder_hidden_state').output, \n",
    "                           model.get_layer('encoder_cell_state').output]])\n",
    "    \n",
    "    # Decoder model\n",
    "    decoder_state_input_h = Input(shape=(HYPER_PARAMS['MODEL_LAYER_DIMS'] * 2, ))\n",
    "    decoder_state_input_c = Input(shape=(HYPER_PARAMS['MODEL_LAYER_DIMS'] * 2, ))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = model.get_layer('decoder_lstm')(decoder_embedding,\n",
    "                                                                        initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_model = Model([model.get_layer('decoder_inputs').output, \n",
    "                           decoder_states_inputs],\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def infer_answer_sentence(model, encoder_model, decoder_model, raw_question, tokenizer, glove_vectors, special_tokens):\n",
    "    \n",
    "    processed_question, tokenized_question = pre_process_new_questions(raw_question, \n",
    "                                                                       tokenizer, \n",
    "                                                                       glove_vectors, \n",
    "                                                                       special_tokens)\n",
    "    tokenized_question = tf.expand_dims(tokenized_question, axis=0)\n",
    "    \n",
    "    # Encode input sentence\n",
    "    encoder_outputs, states = encoder_model.predict(tokenized_question)\n",
    "    \n",
    "    # Create starting input with only <SOS> token\n",
    "    decoder_input = tf.expand_dims(special_tokens['<SOS>'], 0)\n",
    "    \n",
    "    decoded_answer_sequence = []\n",
    "    \n",
    "    while len(decoded_answer_sequence) < HYPER_PARAMS['MAX_SENT_LENGTH']:\n",
    "        decoder_outputs , decoder_hidden_states , decoder_cell_states = decoder_model.predict([decoder_input] + states)\n",
    "        # Apply attention to decoder output\n",
    "        attention_layer = AttentionLayer()\n",
    "        attention_outputs, attention_states = attention_layer([encoder_outputs, decoder_outputs])\n",
    "        decoder_concatenated = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
    "        dense_output = model.get_layer('outputs')(decoder_concatenated)\n",
    "        \n",
    "        # Get argmax of output\n",
    "        sampled_word = np.argmax(dense_output[0, -1, :])\n",
    "        \n",
    "        # Finished output sentence\n",
    "        if sampled_word == special_tokens['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        # Store prediction, and use it as the decoder's new input\n",
    "        decoded_answer_sequence.append(sampled_word)\n",
    "        decoder_input = tf.expand_dims(sampled_word, 0)\n",
    "        \n",
    "        # Update internal states each timestep\n",
    "        states = [decoder_hidden_states, decoder_cell_states]\n",
    "        \n",
    "    answer = post_process_new_answers(decoded_answer_sequence, tokenizer, glove_vectors)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = get_encoder_decoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_question = 'do you want LUNCH'\n",
    "answer = infer_answer_sentence(model, encoder_model, decoder_model, raw_question, tokenizer, glove_vectors, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no no no no no'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram link up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "from telebot import types\n",
    "\n",
    "from spellchecker import SpellChecker #Pip install\n",
    "spell = SpellChecker(distance=2)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<telebot.types.Message at 0x1823326e2e0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API_TOKEN = \"1056118906:AAED4TQl8z-w5vd2Eh8tI6mGRdVGZPs_Fu0\"\n",
    "\n",
    "# bot = telebot.TeleBot(API_TOKEN)\n",
    "\n",
    "# id_=1274801758 #Need to create a ground ID to get this number\n",
    "# welcome=\"Please enter /start to proceed /help\"\n",
    "# bot.send_message(1274801758,text=welcome)\n",
    "# option_dict = []\n",
    "# option_select=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lists to store choices from spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send welcome\n",
      "in For\n",
      "where\n",
      "in For\n",
      "is\n",
      "in For\n",
      "the\n",
      "in For\n",
      "sea\n",
      "sent correct from beg. where is the sea\n",
      "in For\n",
      "do\n",
      "in For\n",
      "you\n",
      "in For\n",
      "sleep\n",
      "sent correct from beg. do you sleep\n",
      "in For\n",
      "why\n",
      "sent correct from beg. why\n",
      "in For\n",
      "are\n",
      "in For\n",
      "you\n",
      "in For\n",
      "human\n",
      "sent correct from beg. are you human\n",
      "in For\n",
      "what\n",
      "in For\n",
      "colour\n",
      "in For\n",
      "is\n",
      "in For\n",
      "the\n",
      "in For\n",
      "sky\n",
      "sent correct from beg. what colour is the sky\n",
      "in For\n",
      "go\n",
      "in For\n",
      "to\n",
      "in For\n",
      "bed\n",
      "sent correct from beg. go to bed\n",
      "in For\n",
      "why\n",
      "sent correct from beg. why\n",
      "in For\n",
      "what\n",
      "in For\n",
      "do\n",
      "in For\n",
      "you\n",
      "in For\n",
      "like\n",
      "in For\n",
      "to\n",
      "in For\n",
      "eat\n",
      "sent correct from beg. what do you like to eat\n",
      "in For\n",
      "do\n",
      "in For\n",
      "you\n",
      "in For\n",
      "drink\n",
      "sent correct from beg. do you drink\n",
      "in For\n",
      "are\n",
      "in For\n",
      "you\n",
      "in For\n",
      "alive\n",
      "sent correct from beg. are you alive\n",
      "in For\n",
      "where\n",
      "in For\n",
      "is\n",
      "in For\n",
      "batman\n",
      "sent correct from beg. where is batman\n",
      "in For\n",
      "what\n",
      "in For\n",
      "is\n",
      "in For\n",
      "your\n",
      "in For\n",
      "favourite\n",
      "in For\n",
      "movie\n",
      "sent correct from beg. What is your favourite movie\n",
      "in For\n",
      "i\n",
      "in For\n",
      "looked\n",
      "in For\n",
      "for\n",
      "in For\n",
      "you\n",
      "in For\n",
      "back\n",
      "in For\n",
      "at\n",
      "in For\n",
      "the\n",
      "in For\n",
      "party,\n",
      "In spell check\n",
      "spell complted with correction needed\n",
      "in select option\n",
      "look up /A\n",
      "Look index A\n",
      "A I looked for you back at the PARTY\n",
      "pulling response I looked for you back at the PARTY\n",
      "anser i am not going to be <UNK>\n",
      "in For\n",
      "where\n",
      "in For\n",
      "are\n",
      "in For\n",
      "the\n",
      "in For\n",
      "dogs\n",
      "sent correct from beg. Where are the dogs\n",
      "in For\n",
      "what\n",
      "in For\n",
      "do\n",
      "in For\n",
      "you\n",
      "in For\n",
      "know\n",
      "sent correct from beg. what do you know\n",
      "in For\n",
      "is\n",
      "in For\n",
      "there\n",
      "in For\n",
      "school\n",
      "in For\n",
      "tomorrow\n",
      "sent correct from beg. is there school tomorrow\n",
      "in For\n",
      "what\n",
      "in For\n",
      "the\n",
      "in For\n",
      "hell\n",
      "in For\n",
      "are\n",
      "in For\n",
      "you\n",
      "sent correct from beg. what the hell are you\n",
      "in For\n",
      "unk\n",
      "In spell check\n",
      "spell complted with correction needed\n",
      "in select option\n",
      "look up /C\n",
      "Look index C\n",
      "C UNK\n",
      "pulling response UNK\n",
      "anser i am not sure\n",
      "in For\n",
      "do\n",
      "in For\n",
      "you\n",
      "in For\n",
      "drive\n",
      "sent correct from beg. do you drive\n"
     ]
    }
   ],
   "source": [
    "API_TOKEN = \"1056118906:AAED4TQl8z-w5vd2Eh8tI6mGRdVGZPs_Fu0\"\n",
    "\n",
    "bot = telebot.TeleBot(API_TOKEN)\n",
    "\n",
    "'''\n",
    "Remove this if you dont have a group set on Telegram\n",
    "'''\n",
    "\n",
    "#https://api.telegram.org/bot1056118906:AAED4TQl8z-w5vd2Eh8tI6mGRdVGZPs_Fu0/getUpdates\n",
    "    \n",
    "    \n",
    "##########################################################################\n",
    "id_=1274801758 #Need to create a ground ID to get this number\n",
    "welcome=\"Please enter /start to proceed /help\"\n",
    "bot.send_message(1274801758,text=welcome)\n",
    "####################################################################\n",
    "\n",
    "option_dict = []\n",
    "option_select=[]\n",
    "\n",
    "@bot.message_handler(commands=['help', 'start'])\n",
    "def send_welcome(message):\n",
    "    print(\"send welcome\")\n",
    "    wel = bot.reply_to(message, \"Welcome to the NLP purpose built bot of PC and SA. I do not function well, so lets see how this goes\")\n",
    "    bot.register_next_step_handler(wel, check_spelling)\n",
    "\n",
    "def check_spelling(message):\n",
    "        try: \n",
    "            words=message.text \n",
    "            \n",
    "            test=True\n",
    "            \n",
    "            sent=words.split()\n",
    "            for i in range(len(sent)):\n",
    "                print(\"in For\")\n",
    "                test2=False\n",
    "                word=sent[i].lower()\n",
    "                check=spell.correction(word)\n",
    "                print(word)\n",
    "\n",
    "                if check==word: #correctly spelled\n",
    "                    continue\n",
    "\n",
    "                if check != word:\n",
    "                    test2=True\n",
    "                    print(\"In spell check\")\n",
    "                    mistake=True\n",
    "                    candidate=list(spell.candidates(word))\n",
    "                    len_candidate=len(candidate)\n",
    "                    if len_candidate>1:\n",
    "                        #Closest match\n",
    "                        correction_one=sent\n",
    "                        correction_one[i]=candidate[0].upper()\n",
    "                        A=' '.join(correction_one)\n",
    "                        #NExt best\n",
    "                        correction_two=sent\n",
    "                        correction_two[i]=candidate[1].upper()\n",
    "                        B=' '.join(correction_two)\n",
    "                        #Original\n",
    "                        C=words\n",
    "                        hold=\"Did you mean: \"+ '\\n'+\"/A)\"+ A + '\\n'+\"/B)\"+B+  '\\n'+\"/C)\"+C\n",
    "                        option_dict.append(A)\n",
    "                        option_dict.append(B)\n",
    "                        option_dict.append(C)\n",
    "\n",
    "                    else:\n",
    "                        correction_one=sent\n",
    "                        correction_one[i]=candidate[0].upper()\n",
    "                        A=' '.join(correction_one)\n",
    "                        B=\"\"\n",
    "                        C=\"\"\n",
    "                        hold=\"Did you mean: \"+ '\\n'+\"/A)\"+ A + '\\n'+\"/B)\"+words\n",
    "                        option_dict.append(A)\n",
    "                        option_dict.append(B)\n",
    "                        option_dict.append(C)\n",
    "\n",
    "            \n",
    "            if test2==True and i==(len(sent)-1):\n",
    "                print(\"spell complted with correction needed\")\n",
    "                msg = bot.reply_to(message, hold)\n",
    "                bot.register_next_step_handler(msg,select_option)\n",
    "                #print(hold)\n",
    "            if test2==False and i==(len(sent)-1):\n",
    "                \n",
    "                print(\"sent correct from beg.\",message.text)\n",
    "                #response=get_response(message.text)\n",
    "               \n",
    "                raw_question = message.text\n",
    "                answer = infer_answer_sentence(model, encoder_model, decoder_model, raw_question, tokenizer, glove_vectors, special_tokens)\n",
    "                response=answer\n",
    "                \n",
    "                msg = bot.reply_to(message, response)\n",
    "                bot.register_next_step_handler(msg, check_spelling)\n",
    "                \n",
    "            \n",
    "            if test==False:\n",
    "                bot.register_next_step_handler(msg,select_option)\n",
    "        except Exception as e:\n",
    "            bot.reply_to(message, 'Its the end of the world part1')\n",
    "            \n",
    "\n",
    "\n",
    "def select_option(message):\n",
    "    print(\"in select option\")\n",
    "    try:\n",
    "        option_list=[\"/A\",\"/B\",\"/C\"]\n",
    "        option = message.text\n",
    "        if option not in option_list:\n",
    "            msg = bot.reply_to(message, 'Please select a valid option')\n",
    "            bot.register_next_step_handler(msg,select_option)\n",
    "            return\n",
    "        option_select.append(option)\n",
    "        \n",
    "        look_up=option_select[-1]\n",
    "        look_index=look_up[-1]\n",
    "        print(\"look up\",look_up)\n",
    "        print('Look index',look_index)\n",
    "        \n",
    "        '''Get response from bot\n",
    "        need CB stuff to link\n",
    "        \n",
    "        '''\n",
    "\n",
    "        if look_index==\"C\":\n",
    "            print(\"C\",option_dict[-1])\n",
    "            raw_question= (option_dict[-1])\n",
    "            \n",
    "        if look_index==\"B\":\n",
    "            print(\"B\",option_dict[-2])\n",
    "            raw_question=(option_dict[-2])\n",
    "            \n",
    "        if look_index==\"A\":\n",
    "            print(\"A\",option_dict[-3])\n",
    "            raw_question= (option_dict[-3])\n",
    "            \n",
    "            \n",
    "        #raw_question= \n",
    "        print(\"pulling response\",raw_question)\n",
    "        answer = infer_answer_sentence(model, encoder_model, decoder_model, raw_question, tokenizer, glove_vectors, special_tokens)\n",
    "        response=answer\n",
    "        print(\"anser\",answer)\n",
    "\n",
    "        #response=\"GET REPLY AGAIN\"\n",
    "        \n",
    "        msg = bot.reply_to(message, response)\n",
    "        \n",
    "        bot.register_next_step_handler(msg, check_spelling)\n",
    "    except Exception as e:\n",
    "        bot.reply_to(message, 'Its the end of the world part2')\n",
    "\n",
    "bot.polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
