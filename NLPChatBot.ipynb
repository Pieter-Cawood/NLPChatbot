{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pybot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import bot_utils\n",
    "from collections import defaultdict\n",
    "from IPython.display import SVG\n",
    "# Keras imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.text as tftext\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from attention import AttentionLayer\n",
    "# Gensim and Spelling\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "from spellchecker import SpellChecker\n",
    "import concurrent.futures\n",
    "# Telegram\n",
    "import logging\n",
    "from threading import Timer\n",
    "from telegram import ReplyKeyboardMarkup, ReplyKeyboardRemove, Update, ChatAction\n",
    "from telegram.ext import (\n",
    "    Updater,\n",
    "    CommandHandler,\n",
    "    MessageHandler,\n",
    "    Filters,\n",
    "    ConversationHandler,\n",
    "    CallbackContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {\n",
    "    'ENABLE_GPU' : True,\n",
    "    'ENABLE_SPELLING_SUGGESTION' : False,\n",
    "    'MESSAGE_TYPING_DELAY' : 4.0,  # Seconds to mimic bot typing\n",
    "    'DATA_BATCH_SIZE' : 128,\n",
    "    'DATA_BUFFER_SIZE' : 10000, \n",
    "    'EMBEDDING_DIMENSION' : 100,  # Dimension of the GloVe embedding vector\n",
    "    'MAX_SENT_LENGTH' : 11,       # Maximum length of sentences\n",
    "    'MAX_SAMPLES' : 2000000,     # Maximum samples to consider (useful for laptop memory)  200000\n",
    "    'MIN_WORD_OCCURENCE' : 30,    # Minimum word count. If condition not met word replaced with <UNK>\n",
    "    'MODEL_LAYER_DIMS' : 500,\n",
    "    'MODEL_LEARN_RATE' : 1e-3,\n",
    "    'MODEL_LEARN_EPOCHS' : 10,\n",
    "    'MODEL_TRAINING' : True # If False - Model weights are loaded from file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPER_PARAMS['ENABLE_GPU']:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data set\n",
    "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "movie_lines:\n",
    "- lineID\n",
    "- characterID (who uttered this phrase)\n",
    "- movieID\n",
    "- character name\n",
    "- text of the utterance\n",
    "\n",
    "movie_conversations:\n",
    "- characterID of the first character involved in the conversation\n",
    "- characterID of the second character involved in the conversation\n",
    "- movieID of the movie in which the conversation occurred\n",
    "- list of the utterances that make the conversation, in chronological \n",
    "order: ['lineID1','lineID2',Ã‰,'lineIDN']\n",
    "has to be matched with movie_lines.txt to reconstruct the actual content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('data/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n",
    "- Perform cleaning\n",
    "- Extract Q&As\n",
    "- Tokenize\n",
    "- Add padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_to_lower(text):\n",
    "    \"\"\"Performs cleaning and lowers text\"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Expand contractions to words, and remove characters\n",
    "    for word in text.split():\n",
    "        if word in bot_utils.CONTRACTIONS:\n",
    "            text = text.replace(word, bot_utils.CONTRACTIONS[word]) \n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "    \n",
    "def get_questions_answers(lines, conversations):\n",
    "    \"\"\"Builds question and answers from data sets\"\"\"\n",
    "    # Map id to text of the utterance\n",
    "    id_to_line = {}\n",
    "    for line in lines:\n",
    "        line_ = line.split(' +++$+++ ')\n",
    "        if len(line_) == 5:\n",
    "            id_to_line[line_[0]] = line_[4]\n",
    "    \n",
    "    questions, answers = [], []\n",
    "    \n",
    "    for line in conversations:\n",
    "        # Extract conversation turns\n",
    "        conversation = line.split(' +++$+++ ')\n",
    "        if len(conversation) == 4:\n",
    "            conversation_turns = conversation[3][1:-1].split(', ')\n",
    "            turns_list = [turn[1:-1] for turn in conversation_turns]   \n",
    "            # Split to Q & A\n",
    "            for i_turn in range(len(conversation_turns) - 1):  \n",
    "                questions.append(clean_text_to_lower(id_to_line[conversation_turns[i_turn][1:-1]]))\n",
    "                answers.append(clean_text_to_lower(id_to_line[conversation_turns[i_turn + 1][1:-1]]))\n",
    "                if len(questions) >= HYPER_PARAMS['MAX_SAMPLES']:\n",
    "                    return questions, answers\n",
    "    return questions, answers\n",
    "\n",
    "def tokenize(lines, conversations):\n",
    "    \"\"\"Tokenizes sets to sequences of integers, and adds special tokens.\n",
    "    Also reduces the vocabulary by replacing low frequency words with an unknown token.\n",
    "    \n",
    "    Returns:\n",
    "    tokenizer: tokenizer,  which might be used later to reverse integers to words,\n",
    "    tokenized_questions: list, questions as sequence of integers\n",
    "    tokenized_answers: list, answer as sequence of integers\n",
    "    size_vocab: int, unique number of words in vocabulary.\n",
    "    special_tokens: dict, mappings for special tokens\"\"\"\n",
    "    \n",
    "    questions, answers = get_questions_answers(lines, conversations)\n",
    "    tokenizer = tftext.Tokenizer(oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(questions + answers)\n",
    "    \n",
    "    # Make UNK tokens, reindex tokenizer dicts\n",
    "    sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    tokenizer.word_index = {}\n",
    "    tokenizer.index_word = {}\n",
    "    index = 1\n",
    "    for word, count in sorted_by_word_count:\n",
    "        if count >= HYPER_PARAMS['MIN_WORD_OCCURENCE']:\n",
    "            tokenizer.word_index[word] = index\n",
    "            tokenizer.index_word[index] = word\n",
    "            index += 1\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = {}\n",
    "    special_tokens['<PAD>'] = 0 \n",
    "    special_tokens['<UNK>'] = len(tokenizer.word_index)\n",
    "    special_tokens['<SOS>'] = special_tokens['<UNK>'] + 1\n",
    "    special_tokens['<EOS>'] = special_tokens['<SOS>'] + 1\n",
    "    \n",
    "    for special_token, index_value in special_tokens.items():\n",
    "        tokenizer.word_index[special_token] = index_value\n",
    "        tokenizer.index_word[index_value] = special_token\n",
    "    \n",
    "    # Tokenize to integer sequences\n",
    "    tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "    tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "    \n",
    "    # Size is equal to the last token's index\n",
    "    size_vocab = special_tokens['<EOS>'] + 1\n",
    "\n",
    "    # Add sentence position tokens\n",
    "    tokenized_questions = [[special_tokens['<SOS>']] + tokenized_question + [special_tokens['<EOS>']]\n",
    "                           for tokenized_question in tokenized_questions]\n",
    "    \n",
    "    tokenized_answers   = [[special_tokens['<SOS>']] + tokenized_answer + [special_tokens['<EOS>']]\n",
    "                           for tokenized_answer in tokenized_answers]\n",
    "    \n",
    "    # Add padding at end so we can use a static input size to the model\n",
    "    tokenized_questions = pad_sequences(tokenized_questions, \n",
    "                                        maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                        padding='post')\n",
    "    tokenized_answers   = pad_sequences(tokenized_answers, \n",
    "                                        maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                        padding='post')\n",
    "    \n",
    "    \n",
    "    return tokenizer, tokenized_questions, tokenized_answers, size_vocab, special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, tokenized_questions, tokenized_answers,\\\n",
    "size_vocab, special_tokens = tokenize(lines, conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tf.data.Dataset** <br>\n",
    "Allows caching and prefetching to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'encoder_inputs': tokenized_questions[:, 1:], # Skip <SOS> token\n",
    "        'decoder_inputs': tokenized_answers[:, :-1] # Skip <EOS> token\n",
    "    },\n",
    "    {\n",
    "        'outputs': tokenized_answers[:, 1:]  # Skip <SOS> token\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(HYPER_PARAMS['DATA_BUFFER_SIZE'])\n",
    "dataset = dataset.batch(HYPER_PARAMS['DATA_BATCH_SIZE'])\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create contextual embedding from GloVe\n",
    "Create embedding layer with out context, from a pre-trained Word2Vec Model from Glove:<br>\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_weights(dimension_embedding):\n",
    "    \"\"\" Load GloVe pre-trained model\"\"\"\n",
    "    path='glove/glove.6B.' + str(dimension_embedding) + 'd.txt'\n",
    "    word_to_vec = {}\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word_to_vec[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "        file.close()\n",
    "    return word_to_vec\n",
    "\n",
    "def load_embedding_weights(word_dictionary, size_vocab, dimension_embedding):\n",
    "    \"\"\"Loads embedding weights from GloVe based on our context\"\"\"\n",
    "    word_to_vec = load_glove_weights(dimension_embedding)\n",
    "    embedding_matrix = np.zeros((size_vocab,\n",
    "                                HYPER_PARAMS['EMBEDDING_DIMENSION']))\n",
    "    for word, index in word_dictionary.items():\n",
    "        embedding_vector = word_to_vec.get(word)\n",
    "        # Word is within GloVe dictionary\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix, word_to_vec\n",
    "\n",
    "def build_embedding_layer(word_dictionary, size_vocab, dimension_embedding, length_input): \n",
    "    \"\"\"Builds a non-trainable embedding layer from GloVe \n",
    "    pre-trained model, based on our context\"\"\"\n",
    "    embedding_matrix, word_to_vec  = load_embedding_weights(word_dictionary, size_vocab, dimension_embedding)\n",
    "    size_vocab = size_vocab\n",
    "    embedding_layer = Embedding(size_vocab,\n",
    "                                dimension_embedding,\n",
    "                                input_length=length_input,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "    return embedding_matrix, word_to_vec, embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, word_to_vec, embedding_layer = build_embedding_layer(tokenizer.word_index, \n",
    "                                                                       size_vocab,\n",
    "                                                                       HYPER_PARAMS['EMBEDDING_DIMENSION'],\n",
    "                                                                       HYPER_PARAMS['MAX_SENT_LENGTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(embedding_layer, length_input, size_vocab, layer_dims):\n",
    "    \"\"\"LSTM Seq2Seq model with attention\"\"\"\n",
    "    length_input = length_input - 1\n",
    "    # \n",
    "    # Encoder,\n",
    "    # Biderectional (RNNSearch) as explained in https://arxiv.org/pdf/1409.0473.pdf \n",
    "    #\n",
    "    encoder_inputs = Input(shape=(length_input, ), name='encoder_inputs')\n",
    "    encoder_embedding = embedding_layer(encoder_inputs)\n",
    "    ecoder_lstm = Bidirectional(LSTM(layer_dims,\n",
    "                                     return_state=True,\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=0.05,\n",
    "                                     recurrent_initializer='glorot_uniform',\n",
    "                                     name='encoder_lstm'),\n",
    "                                name='encoder_bidirectional')\n",
    "    \n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = ecoder_lstm(encoder_embedding)\n",
    "    # For annotating sequences, we concatenate forward hidden state with backward one as explained in top paper\n",
    "    state_h = Concatenate(name='encoder_hidden_state')([forward_h, backward_h])\n",
    "    state_c = Concatenate(name='encoder_cell_state')([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # \n",
    "    # Decoder,\n",
    "    # Unidirectional\n",
    "    #\n",
    "    decoder_inputs = Input(shape=(length_input, ), name='decoder_inputs')\n",
    "    decoder_embedding = embedding_layer(decoder_inputs)\n",
    "    decoder_lstm = LSTM(layer_dims * 2, # to match bidirectional size\n",
    "                        return_state=True,\n",
    "                        return_sequences=True,\n",
    "                        dropout=0.05,\n",
    "                        recurrent_initializer='glorot_uniform',\n",
    "                        name='decoder_lstm')\n",
    "    # Set encoder to use the encoder state as initial states\n",
    "    decoder_output,_ , _, = decoder_lstm(decoder_embedding,\n",
    "                                         initial_state=encoder_states)\n",
    "    \n",
    "    # Attention\n",
    "    attention_layer = AttentionLayer(name='attention_layer')\n",
    "    attention_output, attention_state = attention_layer([encoder_outputs, decoder_output])\n",
    "    decoder_concat = Concatenate(axis=-1)([decoder_output, attention_output])\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(size_vocab, name='outputs', activation='softmax')(decoder_concat)\n",
    "    \n",
    "    return Model([encoder_inputs, decoder_inputs], outputs), decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"574pt\" viewBox=\"0.00 0.00 716.00 636.00\" width=\"646pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 632)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-632 712,-632 712,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1904706294792 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1904706294792</title>\n",
       "<polygon fill=\"none\" points=\"300,-581.5 300,-627.5 495,-627.5 495,-581.5 300,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-600.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"377,-581.5 377,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"377,-604.5 433,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"433,-581.5 433,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-612.3\">[(?, 10)]</text>\n",
       "<polyline fill=\"none\" points=\"433,-604.5 495,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-589.3\">[(?, 10)]</text>\n",
       "</g>\n",
       "<!-- 1903465213768 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1903465213768</title>\n",
       "<polygon fill=\"none\" points=\"395.5,-498.5 395.5,-544.5 611.5,-544.5 611.5,-498.5 395.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435.5\" y=\"-517.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"475.5,-498.5 475.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"475.5,-521.5 531.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"531.5,-498.5 531.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-529.3\">(?, 10)</text>\n",
       "<polyline fill=\"none\" points=\"531.5,-521.5 611.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-506.3\">(?, 10, 100)</text>\n",
       "</g>\n",
       "<!-- 1904706294792&#45;&gt;1903465213768 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1904706294792-&gt;1903465213768</title>\n",
       "<path d=\"M426.465,-581.366C438.851,-571.902 453.459,-560.739 466.606,-550.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"468.749,-553.46 474.57,-544.607 464.499,-547.898 468.749,-553.46\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903465232200 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1903465232200</title>\n",
       "<polygon fill=\"none\" points=\"513,-581.5 513,-627.5 708,-627.5 708,-581.5 513,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551.5\" y=\"-600.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"590,-581.5 590,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"590,-604.5 646,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"646,-581.5 646,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"677\" y=\"-612.3\">[(?, 10)]</text>\n",
       "<polyline fill=\"none\" points=\"646,-604.5 708,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"677\" y=\"-589.3\">[(?, 10)]</text>\n",
       "</g>\n",
       "<!-- 1903465232200&#45;&gt;1903465213768 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1903465232200-&gt;1903465213768</title>\n",
       "<path d=\"M581.262,-581.366C568.759,-571.902 554.013,-560.739 540.742,-550.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"542.788,-547.852 532.703,-544.607 538.563,-553.433 542.788,-547.852\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903465776520 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1903465776520</title>\n",
       "<polygon fill=\"none\" points=\"41.5,-415.5 41.5,-461.5 527.5,-461.5 527.5,-415.5 41.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108\" y=\"-434.8\">Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-415.5 174.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-438.5 230.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"230.5,-415.5 230.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379\" y=\"-446.3\">(?, 10, 100)</text>\n",
       "<polyline fill=\"none\" points=\"230.5,-438.5 527.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"379\" y=\"-423.3\">[(?, 10, 1000), (?, 500), (?, 500), (?, 500), (?, 500)]</text>\n",
       "</g>\n",
       "<!-- 1903465213768&#45;&gt;1903465776520 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1903465213768-&gt;1903465776520</title>\n",
       "<path d=\"M443.947,-498.473C416.066,-488.162 382.688,-475.816 353.693,-465.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"354.635,-461.709 344.042,-461.522 352.206,-468.274 354.635,-461.709\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1904711099080 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1904711099080</title>\n",
       "<polygon fill=\"none\" points=\"197.5,-249.5 197.5,-295.5 517.5,-295.5 517.5,-249.5 197.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-268.8\">LSTM</text>\n",
       "<polyline fill=\"none\" points=\"251.5,-249.5 251.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"251.5,-272.5 307.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"307.5,-249.5 307.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-280.3\">[(?, 10, 100), (?, 1000), (?, 1000)]</text>\n",
       "<polyline fill=\"none\" points=\"307.5,-272.5 517.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-257.3\">[(?, 10, 1000), (?, 1000), (?, 1000)]</text>\n",
       "</g>\n",
       "<!-- 1903465213768&#45;&gt;1904711099080 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1903465213768-&gt;1904711099080</title>\n",
       "<path d=\"M534.801,-498.436C582.706,-462.009 664.194,-387.774 619.5,-332 606.285,-315.509 569.555,-303.361 527.628,-294.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"528.24,-291.1 517.745,-292.548 526.852,-297.962 528.24,-291.1\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903465983688 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1903465983688</title>\n",
       "<polygon fill=\"none\" points=\"350,-332.5 350,-378.5 611,-378.5 611,-332.5 350,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-351.8\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"436,-332.5 436,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"436,-355.5 492,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"464\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"492,-332.5 492,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551.5\" y=\"-363.3\">[(?, 500), (?, 500)]</text>\n",
       "<polyline fill=\"none\" points=\"492,-355.5 611,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551.5\" y=\"-340.3\">(?, 1000)</text>\n",
       "</g>\n",
       "<!-- 1903465776520&#45;&gt;1903465983688 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1903465776520-&gt;1903465983688</title>\n",
       "<path d=\"M337.799,-415.473C362.534,-405.251 392.105,-393.031 417.896,-382.372\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"419.307,-385.576 427.212,-378.522 416.633,-379.107 419.307,-385.576\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903600796360 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1903600796360</title>\n",
       "<polygon fill=\"none\" points=\"71,-332.5 71,-378.5 332,-378.5 332,-332.5 71,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"114\" y=\"-351.8\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"157,-332.5 157,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"157,-355.5 213,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"213,-332.5 213,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.5\" y=\"-363.3\">[(?, 500), (?, 500)]</text>\n",
       "<polyline fill=\"none\" points=\"213,-355.5 332,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272.5\" y=\"-340.3\">(?, 1000)</text>\n",
       "</g>\n",
       "<!-- 1903465776520&#45;&gt;1903600796360 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1903465776520-&gt;1903600796360</title>\n",
       "<path d=\"M261.82,-415.366C252.487,-406.259 241.544,-395.579 231.559,-385.835\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"233.754,-383.086 224.153,-378.607 228.865,-388.096 233.754,-383.086\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903466097480 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1903466097480</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 329,-212.5 329,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"49.5\" y=\"-185.8\">AttentionLayer</text>\n",
       "<polyline fill=\"none\" points=\"99,-166.5 99,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"99,-189.5 155,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"155,-166.5 155,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-197.3\">[(?, 10, 1000), (?, 10, 1000)]</text>\n",
       "<polyline fill=\"none\" points=\"155,-189.5 329,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-174.3\">((?, 10, 1000), (?, 10, 10))</text>\n",
       "</g>\n",
       "<!-- 1903465776520&#45;&gt;1903466097480 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>1903465776520-&gt;1903466097480</title>\n",
       "<path d=\"M146.41,-415.477C107.887,-406.366 73.2967,-394.29 61.5,-379 21.5294,-327.191 84.7392,-257.848 128.465,-219.346\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"130.82,-221.938 136.104,-212.755 126.247,-216.638 130.82,-221.938\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903465983688&#45;&gt;1904711099080 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1903465983688-&gt;1904711099080</title>\n",
       "<path d=\"M446.89,-332.366C432.246,-322.723 414.926,-311.317 399.449,-301.125\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"401.346,-298.184 391.07,-295.607 397.496,-304.03 401.346,-298.184\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903600796360&#45;&gt;1904711099080 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1903600796360-&gt;1904711099080</title>\n",
       "<path d=\"M244.128,-332.366C263.216,-322.455 285.89,-310.682 305.926,-300.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"307.662,-303.321 314.924,-295.607 304.436,-297.109 307.662,-303.321\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1904711099080&#45;&gt;1903466097480 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>1904711099080-&gt;1903466097480</title>\n",
       "<path d=\"M305.017,-249.473C280.767,-239.296 251.798,-227.138 226.48,-216.512\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"227.548,-213.165 216.973,-212.522 224.839,-219.619 227.548,-213.165\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1904711112840 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1904711112840</title>\n",
       "<polygon fill=\"none\" points=\"102.5,-83.5 102.5,-129.5 418.5,-129.5 418.5,-83.5 102.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.5\" y=\"-102.8\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"188.5,-83.5 188.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"188.5,-106.5 244.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-83.5 244.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331.5\" y=\"-114.3\">[(?, 10, 1000), (?, 10, 1000)]</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-106.5 418.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331.5\" y=\"-91.3\">(?, 10, 2000)</text>\n",
       "</g>\n",
       "<!-- 1904711099080&#45;&gt;1904711112840 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>1904711099080-&gt;1904711112840</title>\n",
       "<path d=\"M357.807,-249.241C357.134,-226.669 353.296,-191.521 337.5,-166 330.318,-154.397 319.935,-144.264 309.107,-135.838\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"310.918,-132.826 300.785,-129.732 306.777,-138.47 310.918,-132.826\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903466097480&#45;&gt;1904711112840 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>1903466097480-&gt;1904711112840</title>\n",
       "<path d=\"M190.732,-166.366C201.738,-157.08 214.681,-146.16 226.411,-136.262\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"228.913,-138.731 234.299,-129.607 224.399,-133.381 228.913,-138.731\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1903466652168 -->\n",
       "<g class=\"node\" id=\"node10\"><title>1903466652168</title>\n",
       "<polygon fill=\"none\" points=\"163.5,-0.5 163.5,-46.5 357.5,-46.5 357.5,-0.5 163.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-0.5 214.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-23.5 270.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"270.5,-0.5 270.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-31.3\">(?, 10, 2000)</text>\n",
       "<polyline fill=\"none\" points=\"270.5,-23.5 357.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-8.3\">(?, 10, 6492)</text>\n",
       "</g>\n",
       "<!-- 1904711112840&#45;&gt;1903466652168 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>1904711112840-&gt;1903466652168</title>\n",
       "<path d=\"M260.5,-83.3664C260.5,-75.1516 260.5,-65.6579 260.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"264,-56.6068 260.5,-46.6068 257,-56.6069 264,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, decoder_embedding = seq2seq(embedding_layer, \n",
    "                                   HYPER_PARAMS['MAX_SENT_LENGTH'], \n",
    "                                   size_vocab,\n",
    "                                   HYPER_PARAMS['MODEL_LAYER_DIMS'])\n",
    "\n",
    "optimizer = Adam(learning_rate=HYPER_PARAMS['MODEL_LEARN_RATE'])\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB', dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Optionally load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPER_PARAMS['MODEL_TRAINING']:\n",
    "    model.fit(dataset, epochs=HYPER_PARAMS['MODEL_LEARN_EPOCHS'], batch_size=HYPER_PARAMS['DATA_BATCH_SIZE'])\n",
    "    model.save('backup_{0}.h5'.format(HYPER_PARAMS['MODEL_LEARN_EPOCHS']))\n",
    "else:\n",
    "    model.load_weights('backup_{0}.h5'.format(HYPER_PARAMS['MODEL_LEARN_EPOCHS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions processing\n",
    "- Follow the same tokeninzing approach as before\n",
    "- For words that are not in our original context, we use the GloVe embedding to find the most similar word within our vocabulary. If it is still a weird word, it will be replaced by an unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known_words(sentence_words, glove_word_dictionary, local_word_dictionary, topn=20):\n",
    "    \"\"\"Pre-process the input text, corrects spelling, \n",
    "    get similar words from gensim if input word is out of context\n",
    "    or replace with <UNK> for bad words.\"\"\"\n",
    "    result = []\n",
    "    for word in sentence_words:\n",
    "        \n",
    "        # Correct any spelling mistakes\n",
    "        spell = SpellChecker()\n",
    "        word = spell.correction(word)\n",
    "        \n",
    "        if word in local_word_dictionary:\n",
    "            result.append(word) \n",
    "        else:\n",
    "            # Determine if top match is within our local context\n",
    "            found_sim_word = False\n",
    "            if word in glove_word_dictionary:\n",
    "                similar_words = glove_vectors.similar_by_word(word, topn=topn)\n",
    "                for (sim_word, measure) in similar_words:\n",
    "                    if sim_word in local_word_dictionary.keys():\n",
    "                        found_sim_word = True\n",
    "                        result.append(sim_word)\n",
    "                        break\n",
    "            # Funny word\n",
    "            if not found_sim_word:\n",
    "                result.append('<UNK>')\n",
    "    return result\n",
    "            \n",
    "def pre_process_new_questions(text, tokenizer, glove_vectors, special_tokens):\n",
    "    \"\"\" Process text to words within our context,\n",
    "    and tokenize.\"\"\"\n",
    "    # Pre-process\n",
    "    text_ = clean_text_to_lower(text)\n",
    "    split_text = text_.split(' ')\n",
    "    split_text = get_known_words(split_text, glove_vectors, tokenizer.word_index)\n",
    "    processed_question = \" \".join(split_text)\n",
    "    # Tokenize to sequence of ints\n",
    "    # using original tokenizer\n",
    "    tokenized_question = tokenizer.texts_to_sequences([processed_question])\n",
    "    tokenized_question = [[special_tokens['<SOS>']] + \n",
    "                          tokenized_question[0] + \n",
    "                          [special_tokens['<EOS>']]]\n",
    "\n",
    "    # Padding                                                                     \n",
    "    tokenized_question = pad_sequences(tokenized_question, \n",
    "                                       maxlen=HYPER_PARAMS['MAX_SENT_LENGTH'] - 1, \n",
    "                                       padding='post')                                                                       \n",
    "    return processed_question, tokenized_question[0]\n",
    "\n",
    "def post_process_new_answers(text_sequence, tokenizer, glove_vectors):\n",
    "    answer = []\n",
    "    # Build string from text sequence\n",
    "    for text_id in text_sequence:\n",
    "        if text_id != tokenizer.word_index['<EOS>'] and\\\n",
    "           text_id != tokenizer.word_index['<PAD>']:\n",
    "            word = tokenizer.index_word[text_id]\n",
    "            if len(answer) == 0:\n",
    "                word = word.capitalize()\n",
    "            answer.append(word)\n",
    "    answer = \" \".join(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of correction** <br>\n",
    "Vanish is not within our model's context, but dissapear is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to disappear'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_question, _ = pre_process_new_questions('i wouzld like tso vansish', tokenizer, glove_vectors, special_tokens) \n",
    "processed_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_decoder(model):\n",
    "    \"\"\" Get the encoder and decoder models,\n",
    "        used to do inference of text sequences\"\"\"\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder_model = Model(model.get_layer('encoder_inputs').output,\n",
    "                          [model.get_layer('encoder_bidirectional').output[0],\n",
    "                          [model.get_layer('encoder_hidden_state').output, \n",
    "                           model.get_layer('encoder_cell_state').output]])\n",
    "    \n",
    "    # Decoder model\n",
    "    decoder_state_input_h = Input(shape=(HYPER_PARAMS['MODEL_LAYER_DIMS'] * 2, ))\n",
    "    decoder_state_input_c = Input(shape=(HYPER_PARAMS['MODEL_LAYER_DIMS'] * 2, ))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = model.get_layer('decoder_lstm')(decoder_embedding,\n",
    "                                                                        initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_model = Model([model.get_layer('decoder_inputs').output, \n",
    "                           decoder_states_inputs],\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def infer_answer_sentence(model, encoder_model, decoder_model, raw_question, tokenizer, glove_vectors, special_tokens):\n",
    "    \n",
    "    processed_question, tokenized_question = pre_process_new_questions(raw_question, \n",
    "                                                                       tokenizer, \n",
    "                                                                       glove_vectors, \n",
    "                                                                       special_tokens)\n",
    "    tokenized_question = tf.expand_dims(tokenized_question, axis=0)\n",
    "    \n",
    "    # Encode input sentence\n",
    "    encoder_outputs, states = encoder_model.predict(tokenized_question)\n",
    "    \n",
    "    # Create starting input with only <SOS> token\n",
    "    decoder_input = tf.expand_dims(special_tokens['<SOS>'], 0)\n",
    "    \n",
    "    decoded_answer_sequence = []\n",
    "    \n",
    "    while len(decoded_answer_sequence) < HYPER_PARAMS['MAX_SENT_LENGTH']:\n",
    "        decoder_outputs , decoder_hidden_states , decoder_cell_states = decoder_model.predict([decoder_input] + states)\n",
    "        # Apply attention to decoder output\n",
    "        attention_layer = AttentionLayer()\n",
    "        attention_outputs, attention_states = attention_layer([encoder_outputs, decoder_outputs])\n",
    "        decoder_concatenated = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
    "        dense_output = model.get_layer('outputs')(decoder_concatenated)\n",
    "        \n",
    "        # Get argmax of output\n",
    "        sampled_word = np.argmax(dense_output[0, -1, :])\n",
    "        \n",
    "        # Finished output sentence\n",
    "        if sampled_word == special_tokens['<EOS>']:\n",
    "            break\n",
    "        \n",
    "        # Store prediction, and use it as the decoder's new input\n",
    "        decoded_answer_sequence.append(sampled_word)\n",
    "        decoder_input = tf.expand_dims(sampled_word, 0)\n",
    "        \n",
    "        # Update internal states each timestep\n",
    "        states = [decoder_hidden_states, decoder_cell_states]\n",
    "        \n",
    "    answer = post_process_new_answers(decoded_answer_sequence, tokenizer, glove_vectors)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = get_encoder_decoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface with Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO\n",
    ")\n",
    "\n",
    "updater = Updater(\"1396342801:AAElqHfd2RGJI-lhdbVevFYSIW4HolMjS7E\", use_context=True)\n",
    "# Get the dispatcher to register handlers\n",
    "dispatcher = updater.dispatcher\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WELCOMED_IDS = []\n",
    "MARKUP_MESSAGES = defaultdict(int)\n",
    "\n",
    "def welcome_message(chat_id, context):\n",
    "    context.bot.send_message(chat_id=chat_id, text=\n",
    "    'Hi there! I am NLTChatBot. \\n' +\n",
    "    'I have studied the transcripts of 617 movies to learn to speak.' +\n",
    "    ' Ask me anything! ' + u'\\U0001F603' )\n",
    "    WELCOMED_IDS.append(chat_id)\n",
    "    \n",
    "def check_grammar(message):\n",
    "    _message = message.replace(\"?\", \"\").split(' ')\n",
    "    processed_message = []\n",
    "    highlited_grammar = []\n",
    "    flag = False\n",
    "    for word in _message:\n",
    "        corrected_word = spell.correction(word)\n",
    "        processed_message.append(corrected_word)\n",
    "        if corrected_word != word:\n",
    "            # Bold correction\n",
    "            highlited_grammar.append('<b>'+ word + '</b>')\n",
    "            flag = True\n",
    "        else:\n",
    "            highlited_grammar.append(word)\n",
    "    return ' '.join(highlited_grammar), ' '.join(processed_message), flag\n",
    "\n",
    "def handle_message(update, context): \n",
    "    # Delete old markup option if user didn't select it\n",
    "    if MARKUP_MESSAGES[update.effective_chat.id] != 0:\n",
    "        try:\n",
    "            context.bot.deleteMessage(chat_id = update.effective_chat.id, \n",
    "                                      message_id = MARKUP_MESSAGES[update.effective_chat.id])\n",
    "        # Chat possibly closed\n",
    "        except:\n",
    "            MARKUP_MESSAGES[update.effective_chat.id] = 0\n",
    "        MARKUP_MESSAGES[update.effective_chat.id] = 0\n",
    "        \n",
    "    # New user?\n",
    "    if update.effective_chat.id not in WELCOMED_IDS:\n",
    "        welcome_message(update.effective_chat.id, context)\n",
    "        return\n",
    "    \n",
    "    if HYPER_PARAMS['ENABLE_SPELLING_SUGGESTION']:\n",
    "        # Check grammar\n",
    "        highlited_grammar, processed_message, flag = check_grammar(update.message.text) \n",
    "        if flag:\n",
    "            # Spelling suggestions,\n",
    "            reply_keyboard = [[processed_message]]\n",
    "            message = update.message.reply_text('I am not sure what you meant with - ' + highlited_grammar,\n",
    "                                                 reply_markup=ReplyKeyboardMarkup(reply_keyboard, \n",
    "                                                 one_time_keyboard=True), \n",
    "                                                 parse_mode='HTML')\n",
    "            MARKUP_MESSAGES[update.effective_chat.id] = message.message_id\n",
    "            return\n",
    "    else:\n",
    "        processed_message = update.message.text\n",
    "    \n",
    "    # Reply to user\n",
    "    answer = infer_answer_sentence(model, \n",
    "                                   encoder_model, \n",
    "                                   decoder_model, \n",
    "                                   processed_message, \n",
    "                                   tokenizer, \n",
    "                                   glove_vectors, \n",
    "                                   special_tokens)\n",
    "    \n",
    "    context.bot.send_message(chat_id=update.effective_chat.id, text=answer)\n",
    "\n",
    "def received_message(update, context):\n",
    "    \"\"\"Mimics the typing on keybord,\n",
    "    before calling the message handling\"\"\"\n",
    "    if HYPER_PARAMS['MESSAGE_TYPING_DELAY'] > 0:\n",
    "        context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)\n",
    "        timed_response = Timer(HYPER_PARAMS['MESSAGE_TYPING_DELAY'], handle_message, [update, context])\n",
    "        timed_response.start()\n",
    "    else:\n",
    "        handle_message(update, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<queue.Queue at 0x1bb7de60e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_handler = MessageHandler(Filters.text & (~Filters.command), received_message)\n",
    "dispatcher.add_handler(message_handler)\n",
    "updater.start_polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
